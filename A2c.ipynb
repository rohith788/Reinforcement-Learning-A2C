{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class act_2_critic:\n",
    "    def __init__(self,env):\n",
    "      self.env = env\n",
    "      self.state_size = env.observation_space.shape[0]\n",
    "      self.action_size = env.action_space.shape[0]\n",
    "      self.g = 0.9\n",
    "      self.lr = 0.001\n",
    "      self.memory = deque(maxlen = 2000)\n",
    "      self.actor = self.actor_nn_model()\n",
    "      self.critic = self.critic_nn_model()\n",
    "      self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]\n",
    "    \n",
    "    \n",
    "    def store(self, s, a, r, _s, d):\n",
    "      self.memory.append((s, a, r, _s, d))    \n",
    "        \n",
    "    def actor_nn_model(self):\n",
    "      state = Input(batch_shape=(None, self.state_size))\n",
    "      actor_input = Dense(30, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform')(state)\n",
    "      mu_0 = Dense(self.action_size, activation='tanh', kernel_initializer='he_uniform')(actor_input)\n",
    "      sigma_0 = Dense(self.action_size, activation='softplus', kernel_initializer='he_uniform')(actor_input)\n",
    "      mu = Lambda(lambda x: x * 2)(mu_0) #output 1\n",
    "      sigma = Lambda(lambda x: x + 0.0001)(sigma_0) # output 2\n",
    "      model = Model(inputs = state, outputs = (mu, sigma))\n",
    "      model._make_predict_function()\n",
    "      return  model\n",
    "        \n",
    "        \n",
    "    def act(self, S):\n",
    "      m, sig = self.actor.predict(np.reshape(S, [1, self.state_size]))\n",
    "      ep = np.random.randn(self.action_size)\n",
    "      A = m + np.sqrt(sig) * ep\n",
    "      A = np.clip(A, -2, 2)\n",
    "      return A\n",
    "    \n",
    "    def critic_nn_model(self):\n",
    "      state = Input(batch_shape=(None, self.state_size))\n",
    "      critic_input = Dense(30, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform')(state)\n",
    "      state_value = Dense(1, activation='linear', kernel_initializer='he_uniform')(critic_input)\n",
    "      model = Model(inputs = state, outputs = state_value)\n",
    "      model._make_predict_function()\n",
    "      return  model\n",
    "\n",
    "    def actor_optimizer(self):\n",
    "        #the State, Action and Advantage values are being updated during the train() function\n",
    "        action = K.placeholder(shape=(None, 1)) \n",
    "        advantages = K.placeholder(shape=(None, 1))\n",
    "        mu, sigma_sq = self.actor.output # the joing asction values being predicted from the neural net\n",
    "        #we are using tensors here and so we are are using Tensorflow functions (K.) for the equation below\n",
    "        pdf = 1.0 / K.sqrt(2.0 * np.pi * sigma_sq) * K.exp(-K.square(action - mu) / (2.0 * sigma_sq))\n",
    "        log_pdf = K.log(pdf + K.epsilon())\n",
    "        entropy = K.sum(0.5 * (K.log(2.0 * np.pi * sigma_sq) + 1.0))\n",
    "        exp_v = log_pdf * advantages\n",
    "        exp_v = K.sum(exp_v + 0.01 * entropy)\n",
    "        actor_loss = -exp_v\n",
    "        optimizer = Adam(lr = 0.0001)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], actor_loss)\n",
    "        train = K.function([self.actor.input, action, advantages], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "    def critic_optimizer(self):\n",
    "        #State and target values are being updated from the train() function\n",
    "        discounted_reward = K.placeholder(shape=(None, 1))\n",
    "        value = self.critic.output\n",
    "        loss = K.mean(K.square(discounted_reward - value))\n",
    "        optimizer = Adam(lr = 0.001)\n",
    "        updates = optimizer.get_updates(self.critic.trainable_weights, [], loss)\n",
    "        train = K.function([self.critic.input, discounted_reward], [], updates=updates)\n",
    "        return train\n",
    "\n",
    "        \n",
    "            \n",
    "    def train(self, S, A, R, N_S, done):\n",
    "      state_batch = []\n",
    "    #   We are keepin the batch size as 4 because increasing the batch size is also increasing our tain time considerably.  \n",
    "      if len(self.memory) < 1:\n",
    "        return\n",
    "      sample_batch = random.sample(self.memory, 1)\n",
    "      for S, A, R, N_S, done in sample_batch:\n",
    "        target = np.zeros((1, 1))\n",
    "        advantage = np.zeros((1, self.action_size))\n",
    "        val = self.critic.predict(S)[0]\n",
    "        N_val = self.critic.predict(N_S)[0]\n",
    "\n",
    "        if done:\n",
    "          advantage[0] = R - val\n",
    "          target[0][0] = R\n",
    "      \n",
    "        else:\n",
    "        # print(R.shape, \" --\", N_S.shape, \"------\", val.shape) \n",
    "          advantage[0] = R + self.g * (N_val) - val\n",
    "          target[0][0] = R + self.g * N_val\n",
    "\n",
    "        self.optimizer[0]([S, A, advantage])\n",
    "        self.optimizer[1]([S, target])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "a2c = act_2_critic(env)\n",
    "scores_arr = []\n",
    "epochs = 10000\n",
    "e = []\n",
    "done = False\n",
    "A = env.action_space.sample()\n",
    "end = 0\n",
    "count = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    if count == 50:\n",
    "        print(\"The agent has been trained at - \", i,\" epochs\")\n",
    "        break\n",
    "    S = env.reset()\n",
    "    S = np.reshape(S, [1,env.observation_space.shape[0]])\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        A = a2c.act(S)        \n",
    "        N_S, R, done, info = env.step(A)\n",
    "        N_S = N_S.reshape((1, env.observation_space.shape[0]))\n",
    "        R /= 10\n",
    "        a2c.store(S, A, R, N_S, done)\n",
    "        a2c.train(S, A, R, N_S, done)\n",
    "        score += R\n",
    "        S = N_S\n",
    "        if done:\n",
    "          scores_arr.append(score)\n",
    "          if score > -20:\n",
    "              count += 1\n",
    "          e.append(i)\n",
    "          print(\"epoch: \", i, \"score: \", score)\n",
    "        \n",
    "print(\"the model has been trained in \", i , \" epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores_arr)\n",
    "plt.ylabel(\"epochs\")\n",
    "plt.xlabel(\"rewards\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
